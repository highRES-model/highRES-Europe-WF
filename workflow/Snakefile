configfile: "config/config_ci.yaml"



import itertools

import pandas as pd
import pathlib

date_range = config["parameters"]["date_range"]

user_mode = config["parameters"]["mode"][0]

include_rooftop=config["parameters"]["include_rooftop"]

inputyears = config["parameters"]["years"]

cmip_windspeed = config["parameters"]["cmip_windspeed"]
cmip_solar = config["parameters"]["cmip_solar"]
cmip_runoff = config["parameters"]["cmip_runoff"]
input_cmip_years = config["parameters"]["cmip_years"]
input_cmip_models = config["parameters"]["cmip_models"]
input_cmip_scenarios = config["parameters"]["cmip_scenarios"]
pv_type_pecd = config["parameters"]["pv_type_pecd"]

cmip_windspeed_path = config["parameters"]["cmip_windspeed_path"]
cmip_runoff_path = config["parameters"]["cmip_runoff_path"]
cmip_solar_cf_path = config["parameters"]["cmip_solar_cf_path"]
# TODO: make grid_regions work
grid_regions = []
aggregated_regions = config["parameters"]["aggregated_regions"]

# TODO: worry about zones vs regions later this only works for all aggregated
pd.DataFrame(aggregated_regions).rename(columns={0: "zone"}).to_csv(
    "resources/zones.csv", index=None
)

spatials = config["parameters"]["spatials"]

social_scenario = config["parameters"]["social"]
environmental_scenario = config["parameters"]["environmental"]

psys_scenarios = config["parameters"]["power_system_scenario"]

cutoff_values = {
    "solar": config["parameters"]["cutoff_solar"],
    "onwind": config["parameters"]["cutoff_onwind"],
    "offwind": config["parameters"]["cutoff_offwind"],
}

corine_codes = {
    "solar": config["parameters"]["solar_clc"],
    "onwind_no_buffer": config["parameters"]["onwind_clc_no_buffer"],
    "onwind_buffer": config["parameters"]["onwind_clc_buffer"],
}

windturbines = config["parameters"]["windturbines"]
wind_smooth = config["parameters"]["wind_smooth"]
windon_af = config["parameters"]["windon_af"]
windoff_bottom_af = config["parameters"]["windoff_bottom_af"]

# Generate all combinations of years, spatials, social, and environmental scenarios, cmip_years, cmip_models
combinations = list(itertools.product(psys_scenarios, inputyears, spatials, social_scenario, environmental_scenario,input_cmip_scenarios,input_cmip_years,input_cmip_models))
scenarios = pd.DataFrame(combinations, columns=["psys_scenario", "year", "spatial", "social", "environmental","cmip_scenarios","cmip_years","cmip_models"])

# # cmip years
# concatlist = []
# for cmip_year_value in input_cmip_years:
#     concatlist.append(scenarios.assign(cmip_years=cmip_year_value))
# scenarios = pd.concat(concatlist).reset_index(drop=True)

# # cmip models
# concatlist = []
# for cmip_model_name in input_cmip_models:
#     concatlist.append(scenarios.assign(cmip_models=cmip_model_name))
# scenarios = pd.concat(concatlist).reset_index(drop=True)

psys_scenarios = scenarios.psys_scenario.to_list()
years = scenarios.year.to_list()
spatials = scenarios.spatial.to_list()
socials = scenarios.social.to_list()
environmentals = scenarios.environmental.to_list()
cmip_years = scenarios.cmip_years.to_list() # cmip years
cmip_models = scenarios.cmip_models.to_list() # cmip models
cmip_scenarios = scenarios.cmip_scenarios.to_list() # cmip models

bias_correction = config["parameters"]["bias_correction"]
elevation_excl = config["parameters"]["elevation_excl"]
onshore_slope_excl = config["parameters"]["onshore_slope_excl"]

wimby_outputs=config["parameters"]["wimby_outputs"]
wimby_outputs_hourly=config["parameters"]["wimby_outputs_hourly"]

# Get current working directory

cwd = pathlib.Path().resolve()

# Absolute path to your GAMS installation
gamspath = config["paths"]["gams"]
gamspath = str(gamspath)

# Absolute path to the input data that is the same across all versions
shared_input_path = cwd / pathlib.Path(config["paths"]["shared_input"])

# Relative path the model code that is the same across all versions
shared_code_path = config["paths"]["abs_shared_code"]

abs_gams_code_path = cwd / pathlib.Path(config["paths"]["abs_shared_code"])

# Relative path to the geodata files that differentiate the scenarios
scenario_exclusions_path = "resources/scenario_exclusions/"

# Relative path to the results
# TODO: Vetle: --directory, flag for Ã¥ sette results directory?
# resultspath = "results/"
resultspath = cwd / pathlib.Path(config["paths"]["results"])
pathlib.Path(resultspath).mkdir(parents=True, exist_ok=True)

# Write scenarios to file, so results analysis script can check if they are all
# available.
#scenarios.to_csv(resultspath / "scenarios.csv", sep="\t")

run_name=["psys_scenario","year","spatial","social","environmental"]

scenarios["run_name"]=scenarios[run_name].stack().groupby(level=0).agg("_".join)

scenarios.loc[:,["year","run_name"]].to_csv(resultspath / "scens.csv",index=False,header=False)

# Relative path to workdir

# modelpath=resultspath / "{psys_scenario}_{year}_{spatial}_{socials}_{environmentals}" / "{cmip_year}" / "{cmip_model}"
modelpath=resultspath / "{psys_scenario}_{year}_{spatial}_{socials}_{environmentals}" / "{cmip_scenario}" / "{cmip_year}" / "{cmip_model}"


localrules:
    all,
    build_cplex_opt,
    build_shapes,
    build_technoeconomic_inputs,
    build_vre_cf_grid,
    build_vre_land_avail,
    build_vre_cf_inputs,
    build_zones_file,
    build_vre_areas_file,
    build_vre_file,
    build_vre_gdx,
    build_hydro_capfac,
    build_hydrores_inflow,
    link_hydrores_inflow,
    convert_results,
    build_vre_parquet,
    build_vre_csv,
    compress_vre_gdx,
    compress_hydrores_inflow,
    convert_results_db_parquet,
    # run_gams,


rule all:
    input:
        expand(modelpath / config["target"], zip, psys_scenario=psys_scenarios, year=years, spatial=spatials, socials=socials, environmentals=environmentals, cmip_scenario=cmip_scenarios, cmip_year=cmip_years, cmip_model=cmip_models),


rule build_cplex_opt:
    output:
        modelpath / "cplex.opt",
    params: config["cplex_options"],
    run:
        cplex_options = config["cplex_options"]

        # Open the file in write mode
        with open(output[0], "w") as f:
            for option, value in cplex_options.items():
                f.write(f"{option} {value}\n")


rule build_zones_file:
    input:
        "resources/zones.csv",
        "workflow/scripts/build_zones.py",
    output:
        modelpath / "zones.dd",
    script:
        "scripts/build_zones.py"


rule build_technoeconomic_inputs:
    input:
        "resources/zones.csv",
        "resources/highres_scenarios.xls",
        "resources/highres_technoeconomic_database.ods",
        # europedemandcsvlocation="resources/demand_wimby/{socials}_{environmentals}_demand_{year}.csv",
        europedemandcsvlocation="resources/demand_wimby/europe_demand.csv",
        # co2_targets_db="resources/TIMES_250513_co2_targets.csv",
        co2_targets_db="resources/europe_co2_targets.csv",
        data2dd="workflow/scripts/data2dd_funcs.py",
    conda:
        "envs/build_technoeconomic.yml"
    output:
        modelpath / "{year}_temporal.dd",
        modelpath / "{psys_scenario}_gen.dd",
        modelpath / "{psys_scenario}_store.dd",
        modelpath / "trans.dd",
        co2_target=modelpath / "BASE_co2_target.dd",
        demandfile=modelpath / "BASE_demand_{year}.dd",
    params:
        aggregated_regions=aggregated_regions,
        date_range=date_range,
        user_mode=user_mode,
        co2_target_type=config["parameters"]["co2_target_type"],
        co2_target_extent=config["parameters"]["co2_target_extent"],
        co2_target_scenario="{socials}_{environmentals}",
    script:
        "scripts/data2dd.py"


rule build_shapes:
    input:
        euroshape=shared_input_path
        / "geodata/onshore/shapes/NUTS_RG_01M_2021_4326.geojson",
        eurooffshoreshape=shared_input_path
        / ("geodata/offshore/BOTTOM_MOUNTED_EUROPE_NUTS0" "_NORWAY_NUTS3.geojson"),
    output:
        onshoreshape="intermediate_data/{spatial}/shapes/europe_onshore.geojson",
        offshoreshape="intermediate_data/{spatial}/shapes/europe_offshore.geojson",
    params:
        aggregated_regions=aggregated_regions,
    notebook:
        "notebooks/highRES-build_shapes.ipynb"


rule build_vre_cf_grid:
    input:
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
        euroshape=rules.build_shapes.output.onshoreshape,
        eurooffshoreshape=rules.build_shapes.output.offshoreshape,
        biaswinddata=(
            shared_input_path / "weatherdata/ratio_gwa2_era5.nc"
            if bias_correction
            else []
        ),
    output:
        cf_file="intermediate_data/{spatial}/{cmip_scenario}/{cmip_model}/{cmip_year}/weather/europe_cf_{year}.nc",
    resources:
        mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
    threads: workflow.cores * 0.75
    params:
        sharedinputpath=shared_input_path,
        aggregated_regions=aggregated_regions,
        bias_correction=bias_correction,
        windturbines=windturbines,
        windspeed=cmip_windspeed,
        solar=cmip_solar,
        windspeed_path=cmip_windspeed_path,
        solar_cf_path=cmip_solar_cf_path,
        # cmip_model=lambda wildcards: wildcards.cmip_model,
        # cmip_year=lambda wildcards: wildcards.cmip_year,
        # cmip_scenario=lambda wildcards: wildcards.cmip_scenario,
        pv_type_pecd = pv_type_pecd,
        wind_smooth=wind_smooth,
        windon_af=windon_af,
        windoff_bottom_af=windoff_bottom_af,
    notebook:
        "notebooks/highRES-build_vre_cf_grid.ipynb"


rule build_vre_land_avail:
    input:
        solar_env_excl= shared_input_path / "geodata/onshore/solarpv_CDDA_IUCN_Ia-IV_Natura.tif",
        solar_agr_excl= shared_input_path / "geodata/onshore/solar_agri_excl_med_high_intens.tif",
        solar_slope= shared_input_path / "geodata/onshore/solar_slope_max_6.3deg.tif",
        corine=shared_input_path / "geodata/onshore/corine.tif",
        elevation=shared_input_path / "geodata/onshore/2000m.shp.zip",
        onshore_slope=shared_input_path / "geodata/onshore/onshore_slope_max_15.0deg.tif",
        social_excl=shared_input_path / "geodata/wimby/social_{socials}.tif",
        env_excl=shared_input_path / "geodata/wimby/environmental_{environmentals}.tif",
        techn_excl=shared_input_path / "geodata/wimby/technical_exclusions.tif",
        euroshape=rules.build_shapes.output.onshoreshape,
        eurooffshoreshape=rules.build_shapes.output.offshoreshape,
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
        cfdata=rules.build_vre_cf_grid.output.cf_file,
    output:
        # FIXME the file indreg is only necessary when we run on grid cell
        # currently when we run with region, we create an empty file
        # the cleaner solution might be to outsource the creation of that file
        # to an extra rule and have an input function conditional on the
        # wildcard spatial for that rule
        cf_exclusion_solar=(
            modelpath / "cf_exclusion_solar.tif"
            if user_mode == "developer"
            else temp(modelpath / "cf_exclusion_solar.tif")
        ),
        cf_exclusion_windon=(
            modelpath / "cf_exclusion_windon.tif"
            if user_mode == "developer" and cutoff_values["onwind"] > 0.
            else temp(modelpath / "cf_exclusion_windon.tif")
            if cutoff_values["onwind"] > 0.
            else []
        ),
        cf_exclusion_windoff=(
            modelpath / "cf_exclusion_windoff.tif"
            if user_mode == "developer" and cutoff_values["offwind"] > 0.
            else temp(modelpath / "cf_exclusion_windoff.tif")
            if cutoff_values["offwind"] > 0.
            else []
        ),
        indreg=(
            modelpath / "indices_region.csv"
            if user_mode == "developer"
            else temp(modelpath / "indices_region.csv")
        ),
        grid_areas=modelpath / "grid_areas.csv",
    resources:
        mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
    threads: workflow.cores * 0.75
    params:
        sharedinputpath=shared_input_path,
        aggregated_regions=aggregated_regions,
        cutoffs=cutoff_values,
        corine_codes=corine_codes,
        elevation_excl=elevation_excl,
        onshore_slope_excl=onshore_slope_excl,
        include_rooftop=include_rooftop,
    notebook:
        "notebooks/highRES-build_vre_land_avail.ipynb"


rule build_vre_cf_inputs:
    input:
        cf_file=rules.build_vre_cf_grid.output.cf_file,
        grid_areas=rules.build_vre_land_avail.output.grid_areas,
        rooftop_areas="resources/rooftop_pv_km2.csv",
    output:
        capfacfile=(
            modelpath / "capacity-factors_solar_{year}.csv"
            if user_mode == "developer"
            else temp(modelpath / "capacity-factors_solar_{year}.csv")
        ),
        areassolar=(
            modelpath / "areas_solar.csv"
            if user_mode == "developer"
            else temp(modelpath / "areas_solar.csv")
        ),
        areaswindonshore=(
            modelpath / "areas_wind_onshore.csv"
            if user_mode == "developer"
            else temp(modelpath / "areas_wind_onshore.csv")
        ),
        areaswindoffshore=(
            modelpath / "areas_wind_offshore.csv"
            if user_mode == "developer"
            else temp(modelpath / "areas_wind_offshore.csv")
        ),
        wimby_outputs=(
            modelpath / "wimby_outputs_{socials}_{environmentals}.csv"
            if wimby_outputs
            else []
        ),
    params:
        date_range=date_range,
        include_rooftop=include_rooftop,
        wimby_outputs=wimby_outputs,
        wimby_outputs_hourly=wimby_outputs_hourly,
    notebook:
        "notebooks/highRES-build_vre_cf_inputs.ipynb"


rule build_hydro_capfac:
    input:
        eiahydrogen="resources/EIA_hydro_generation_1995_2000_2014.csv",
        jrchydro="resources/jrc-hydro-power-plant-database.csv",
        euroshape=rules.build_shapes.output.onshoreshape,
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
    output:
        hydrororcapfac=(
            modelpath / "capacity-factors_hydro_{year}.csv"
            if user_mode == "developer"
            else temp(modelpath / "capacity-factors_hydro_{year}.csv")
        ),
        hydroresinfl=(
            modelpath / "inflow_hydro-res_{year}.csv"
            if user_mode == "developer"
            else temp(modelpath / "inflow_hydro-res_{year}.csv")
        ),
        areashydro=(
            modelpath / "hydro_ror_area.csv"
            if user_mode == "developer"
            else temp(modelpath / "hydro_ror_area.csv")
        ),
    conda:
        "envs/build_hydro_capfac.yml"
    params:
        sharedinputpath=shared_input_path,
        aggregated_regions=aggregated_regions,
        date_range=date_range,
        runoff=cmip_runoff,
        runoff_path=cmip_runoff_path,
    notebook:
        "notebooks/highRES_build_hydro.py.ipynb"


def regionsonlyforgrid(wildcards):
    returndict = {}
    if wildcards.spatial == "grid":
        returndict["indreg"] = rules.build_vre_land_avail.output.indreg
    return returndict


rule build_vre_areas_file:
    input:
        areashydro=rules.build_hydro_capfac.output.areashydro,
        areassolar=rules.build_vre_cf_inputs.output.areassolar,
        areaswindon=rules.build_vre_cf_inputs.output.areaswindonshore,
        areaswindoff=rules.build_vre_cf_inputs.output.areaswindoffshore,
        data2dd=pathlib.Path(workflow.basedir) / "scripts",
        technoeconomic_database="resources/highres_technoeconomic_database.ods",
    output:
        modelpath / "vre_areas_{year}_.dd",
        regionsdd=modelpath / "_regions.dd",
    notebook:
        "notebooks/highRES_build_vre_areas_file.ipynb"


rule build_hydrores_inflow:
    input:
        rules.build_hydro_capfac.output.hydroresinfl,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",
    shell:
        gamspath + (
            "csv2gdx {input} output={output} ID=hydro_inflow "
            "Index='(1,2,3)' Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_hydrores_inflow:
    input:
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        compressdone=touch(modelpath / "hydrores_inflow_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


rule link_hydrores_inflow:
    input:
        rules.compress_hydrores_inflow.output.compressdone,
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",
    run:
        # copy instead of link due to missing privileges on windows
        import shutil

        shutil.copy2(input[1], output[0])


rule build_vre_file:
    input:
        cfwsng=rules.build_vre_cf_inputs.output.capfacfile,
        cfhn3=rules.build_hydro_capfac.output.hydrororcapfac,
    output:
        vrefile=(
            modelpath / "vre_{year}_.csv"
            if user_mode == "developer"
            else temp(modelpath / "vre_{year}_.csv")
        ),
    run:
        import shutil

        with open(output.vrefile, "wb") as wfd:
            for f in [
                input["cfwsng"],
                input["cfhn3"],
            ]:
                with open(f, "rb") as fd:
                    shutil.copyfileobj(fd, wfd)
                    # shell:
                    #     # TODO platform independence
                    #     (
                    #         "cat {input[cfwsng]} {input[cfhn3]} | sed 's/bottom//'"
                    #         " > {output[vrefile]}"
                    #     )



rule build_vre_parquet:
    input:
        rules.build_vre_file.output.vrefile,
    output:
        modelpath / "vre_{year}_.parquet",
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_parquet.py"


rule build_vre_csv:
    input:
        modelpath / "vre_{year}_.parquet",
    output:
        csvgdx=(
            modelpath / "vre_{year}_tmp.csv"
            if user_mode == "developer"
            else temp(modelpath / "vre_{year}_tmp.csv")
        ),
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_csv.py"


rule build_vre_gdx:
    input:
        rules.build_vre_csv.output.csvgdx,
    output:
        bigvregdx=(
            modelpath / "vre_{year}_.gdx"
            if user_mode == "developer"
            else modelpath / "vre_{year}_.gdx"
        ),
    shell:
        # TODO platform independence
        gamspath + (
            "csv2gdx {input} output={output} ID=vre_gen Index='(1,2,3)'"
            " Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_vre_gdx:
    input:
        rules.build_vre_gdx.output.bigvregdx,
    output:
        touch(modelpath / "vre_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


""" def inputfilelist(wildcards):
    returndict = {}
    returndict['regionsfile'] = modelpath / "_regions.dd"
    returndict['zonesfile'] = modelpath / "zones.dd"
    returndict['temporalfile'] = modelpath / "{year}_temporal.dd"
    returndict['vreareafile'] = modelpath / "vre_areas_{year}_.dd"
    returndict['demandfile'] = modelpath / "BASE_demand_{year}.dd"
    returndict['capfacfilecompressed'] = modelpath / "vre_gdx.compressed"
    returndict['capfacfile'] = modelpath / "vre_{year}_.gdx"
    returndict['hydroresinflowfile'] = modelpath / "hydro_res_inflow_{year}.gdx"
    returndict['co2budgetfile'] = modelpath / "BASE_co2_budget.dd"
    returndict['genparamsfile'] = modelpath / "BASE_gen.dd"
    returndict['storeparamsfile'] = modelpath / "BASE_store.dd"
    returndict['transparamsfile'] = modelpath / "trans.dd"
    return returndict """


# This rule exists so users can stop the workflow after the input creation, to
# not have it automatically run the model as well, in case this is not desired.
rule build_inputs:
    input:
        # unpack(inputfilelist)
        modelpath / "_regions.dd",
        modelpath / "zones.dd",
        modelpath / "{year}_temporal.dd",
        modelpath / "vre_areas_{year}_.dd",
        rules.build_technoeconomic_inputs.output.demandfile,
        modelpath / "vre_gdx.compressed",
        modelpath / "vre_{year}_.gdx",
        modelpath / "hydro_res_inflow_{year}.gdx",
        modelpath / "{psys_scenario}_gen.dd",
        modelpath / "{psys_scenario}_store.dd",
        modelpath / "trans.dd",
        modelpath / "cplex.opt",
    output:
        touch(modelpath / "inputs.finished"),


# Files that are marked temporary upon creation, but can only be deleted after
# the model has run, need to be listed as an input both here and in the rule
# "build_inputs". If they are only listed in "build_inputs", but not here, they
# would be deleted before the model runs and so the run breaks.
rule run_gams:
    input:
        shared_code_path + "highres.gms",
        shared_code_path + "highres_data_input.gms",
        shared_code_path + "highres_hydro.gms",
        shared_code_path + "highres_results.gms",
        shared_code_path + "highres_storage_setup.gms",
        shared_code_path + "highres_storage_uc_setup.gms",
        shared_code_path + "highres_uc_setup.gms",
        modelpath / "inputs.finished",
        modelpath / "vre_{year}_.gdx",
    params:
        gamspath=gamspath,
        modelpath=str(modelpath),
        #co2intensity=config["parameters"]["co2target"],
        co2_target_type=config["parameters"]["co2_target_type"],
        co2_target_extent=config["parameters"]["co2_target_extent"],
        sharedcodepath=abs_gams_code_path,
        outname="results",
        store_initial_level=config["parameters"]["store_initial_level"],
        store_final_level=config["parameters"]["store_final_level"],
        hydro_res_initial_fill=config["parameters"]["hydro_res_initial_fill"],
        hydro_res_min=config["parameters"]["hydro_res_min"],
        pgen=config["parameters"]["pgen"],
        emis_price=config["parameters"]["emis_price"],
        trans_inv=config["parameters"]["trans_inv"],
        trans_cap_lim=config["parameters"]["trans_cap_lim"],
    resources:
        mem_mb=50000,
    log:
        str(modelpath) + "/highres.lst",
        str(modelpath) + "/highres.log",
    output:
        modelresults=protected(modelpath / "results.gdx"),
        # modelresultsdd=protected(modelpath / "results.db"),
    script:
        "scripts/run_gams.py"


rule convert_results:
    input:
        rules.run_gams.output.modelresults,
    output:
        resultsdb=ensure(protected(modelpath / "results.db"), non_empty=True),
    shell:
        # TODO platform independence
        gamspath + "gamstool sqlitewrite gdxIn={input} o={output}"
        # gamspath + "gdx2sqlite -i {input} -o {output} -fast"


rule convert_results_db_parquet:
    input:
        rules.convert_results.output.resultsdb,
    output:
        compressdone=touch(modelpath / "results.db.compressed"),
    resources:
        mem_mb=50000,
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/convert_results_sqlite_parquet.py"
