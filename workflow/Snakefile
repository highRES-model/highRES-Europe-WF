# snakemake --resources mem_mb=700000 --slurm --default-resources \
# slurm_account=ec85 runtime=4320 mem_mb_per_cpu=21000 cpus_per_task=4 \
# --use-conda --cluster-cancel scancel --keep-going -j 120

# region:
# snakemake --resources mem_mb=700000 --slurm --default-resources \
# slurm_account=ec85 runtime=30 mem_mb_per_cpu=1100 cpus_per_task=4 \
# --use-conda --cluster-cancel scancel --keep-going -j 120


# TODO: Default profile is Fox, HPC, profile


configfile: "config/config_james.yaml"


import itertools

import pandas as pd
import pathlib

date_range=["2010-01-01","2010-12-31"]


inputyears = [
    # "1995",
    "2010"
]

# TODO: make grid_regions work
grid_regions = []
aggregated_regions = [
    "AT",
    "BE",
    "BG",
    "CH",
    "CZ",
    "DE",
    "DK",
    "EE",
    "ES",
    "FI",
    "FR",
    "UK",
    "GR",
    "HR",
    "HU",
    "IE",
    "IT",
    "LT",
    "LU",
    "LV",
    "NL",
    "NO",
    "PL",
    "PT",
    "RO",
    "SE",
    "SI",
    "SK",
]

# TODO: worry about zones vs regions later this only works for all aggregated
pd.DataFrame(aggregated_regions).rename(columns={0: "zone"}).to_csv(
    "resources/zones.csv", index=None
)
# TODO: maybe at some stage remove the need for this redundant file
pd.DataFrame(aggregated_regions).assign(
    Country="",
    ISO3="",
    ETM=1,
    ETM_reg="",
    EU_simpl="",
    gb_ext="",
    UTC="",
    JRC_EUTIMES="",
).rename(columns={0: "ISO2"}).loc[
    :,
    [
        "Country",
        "ISO3",
        "ISO2",
        "ETM",
        "ETM_reg",
        "EU_simpl",
        "gb_ext",
        "UTC",
        "JRC_EUTIMES",
    ],
].to_csv(
    "resources/europe_countries.csv", index=None
)

spatials = [
    # "grid",
    "region",
	#"nuts2"
]

cutoff_values = {"solar": 0.09, "onwind": 0.15, "offwind": 0.20}

scenarios = pd.DataFrame(inputyears, columns=["years"])

concatlist = []
for spatial in spatials:
    concatlist.append(scenarios.assign(spatials=spatial))
scenarios = pd.concat(concatlist).reset_index(drop=True)

years = scenarios.years.to_list()
spatials = scenarios.spatials.to_list()


# in grams per kWh
co2target = round(0, 1)

# gamsfilesha256 = "d8e033699604d4474a1c9db59140749509f30383dee532c1e85165617620776b"
# gamsfilesha256 = "af4a97e52ff41b137a24fb517b03e0e8268d879b01c7aefd7cae3074467ebad1"
# gamsfilesha256 = "b8ff95b0bc41fa7bd18ff192eac69b54ead25d72a851656c0d9e8fe41ca50177"
# gamsfilesha256 = "9b1b86c582b18822fcf9c65045f9c68605e951957cd934cc1713f8cf00a5f375"
# gamsfilesha256 = "7c58219c0ab4320c806c0460ebba547c10838313c6a1f10f6e6640aab90a4838"
# gamsfilesha256 = "9cced6b80dad705f6963e4dee1c48af4b8170d42444bb269b049ea8627d6c0e5"
gamsfilesha256 = "686cc7b751384f1f0401289c1da84f8454221b7eb146bf1b65eb5dae2d08c22a"
# Get current working directory

cwd = pathlib.Path().resolve()

# Absolute path to your GAMS installation
gamspath = config["paths"]["gams"]
gamspath = str(gamspath)

# Absolute path to the input data that is the same across all versions
shared_input_path = cwd / pathlib.Path(config["paths"]["shared_input"])

# Relative path the model code that is the same across all versions
shared_code_path = "resources/4_model_code_shared/"
abs_shared_code_path = config["paths"]["abs_shared_code"]
# abs_shared_code_path = workflow.source_path("../resources/4_model_code_shared/")
# TODO: Get this to work, maybe with Pathlib

# Relative path to the geodata files that differentiate the scenarios
scenario_exclusions_path = "resources/scenario_exclusions/"

# Relative path to the results
# TODO: Vetle: --directory, flag for Ã¥ sette results directory?
# resultspath = "results/"
resultspath = cwd / pathlib.Path(config["paths"]["results"])
pathlib.Path(resultspath).mkdir(parents=True, exist_ok=True)

# Write scenarios to file, so results analysis script can check if they are all
# available.
scenarios.to_csv(resultspath / "scenarios.csv", sep="\t")

# Relative path to the built model and built inputs
yearonlypart = "models/{year}/"
scenariopart = "{spatial}/"
modelpathyearonly = resultspath / yearonlypart
modelpath = modelpathyearonly / scenariopart
allyearpath = resultspath / "models/allyears/"
logpath = yearonlypart + scenariopart

localrules:
    all,
    build_gams,
    build_cplex_opt,
    build_technoeconomic_inputs,
    ensure_gams_template,
    rename_demand_file,
    build_regions_file,
    build_zones_file,
    build_vre_areas_file,
    build_vre_file,
    build_vre_gdx,
    build_hydro_capfac,
    build_hydrores_inflow,
    link_hydrores_inflow,
    convert_results,
    build_vre_parquet,
    build_vre_csv,
    compress_vre_gdx,
    compress_hydrores_inflow,
    convert_results_db_parquet,


rule all:
    input:
        expand(modelpath / "results.db.compressed", zip, year=years, spatial=spatials),


rule ensure_gams_template:
    input:
        "resources/highresraw.gms",
    output:
        gamsfile=ensure(resultspath / "highres.gms", sha256=gamsfilesha256),
    run:
        import shutil

        shutil.copy2(input[0], output[0])


rule build_gams:
    input:
        rules.ensure_gams_template.output.gamsfile,
    params:
        co2intensity=co2target,
        sharedcodepath=abs_shared_code_path,
    output:
        modelpath / "highres.gms",
    script:
        "scripts/build_gams.py"


rule build_cplex_opt:
    input:
        "resources/cplex.opt",
        #ancient("resources/cplex.opt"), # TODO: What does this line do?
    output:
        modelpath / "cplex.opt",
    run:
        import shutil

        shutil.copy2(input[0], output[0])


rule build_zones_file:
    input:
        "resources/zones.csv",
        "workflow/scripts/build_zones.py",
    output:
        modelpath / "zones.dd",
    script:
        "scripts/build_zones.py"


rule build_technoeconomic_inputs:
    input:
        "resources/zones.csv",
        "resources/gb_ext_scenarios.xls",
        "resources/highres_gb_ext_database.ods",
        europecountriescsvlocation="resources/europe_countries.csv",
        europedemandcsvlocation="resources/europe_demand_2006-2015.csv",
        data2dd="workflow/scripts/data2dd_funcs.py",
    conda:
        "envs/build_technoeconomic.yml"
    output:
        modelpath / "{year}_temporal.dd",
        modelpath / "BASE_co2_budget.dd",
        modelpath / "BASE_gen.dd",
        modelpath / "BASE_store.dd",
        modelpath / "trans.dd",
        demandfile=temp(modelpath / "BASE_norescale_demand_{year}.dd"),
    params:
        aggregated_regions=aggregated_regions,
        date_range=date_range,
    script:
        "scripts/gb_ext_data2dd.py"


rule rename_demand_file:
    input:
        rules.build_technoeconomic_inputs.output.demandfile,
    output:
        modelpath / "BASE_demand_{year}.dd",
    run:
        import shutil

        shutil.copy2(input[0], output[0])


# rule build_hydro_areas:
#    input:
#        "resources/zones.csv",
#    output:
#        areashydro=temp(allyearpath / "available_area.txt"),
#    script:
#        "scripts/build_hydro_areas.py"


rule build_vre_cf_grid:
	input:
		weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
		euroshape=shared_input_path
		/ "geodata/onshore/shapes/NUTS_RG_60M_2021_4326.geojson",
		eurooffshoreshape=shared_input_path
		/ ("geodata/offshore/BOTTOM_MOUNTED_EUROPE_NUTS0" "_NORWAY_NUTS3.geojson"),
	output:
		cf_file=shared_input_path / "intermediate_data/europe_cf_{year}.nc",
	resources:
		mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
	threads: workflow.cores * 0.75
	params:
		sharedinputpath=shared_input_path,
		aggregated_regions=aggregated_regions,
	notebook:
		"notebooks/highRES-build_vre_cf_grid.ipynb"
		
rule build_vre_land_avail:
	input:
		WDPA1a=shared_input_path / "geodata/onshore/WDPA_Ia_100.tiff",
		WDPA1b=shared_input_path / "geodata/onshore/WDPA_Ib_100.tiff",
		WDPA2=shared_input_path / "geodata/onshore/WDPA_II_100.tiff",
		WDPA3=shared_input_path / "geodata/onshore/WDPA_III_100.tiff",
		WDPA4=shared_input_path / "geodata/onshore/WDPA_IV_100.tiff",
		elevation=shared_input_path / "geodata/onshore/2000m.shp.zip",
		slope=shared_input_path / "geodata/onshore/15degrees.shp.zip",
		euroshape=shared_input_path
		/ "geodata/onshore/shapes/NUTS_RG_60M_2021_4326.geojson",
		eurooffshoreshape=shared_input_path
		/ ("geodata/offshore/BOTTOM_MOUNTED_EUROPE_NUTS0" "_NORWAY_NUTS3.geojson"),
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
		cfdata=rules.build_vre_cf_grid.output.cf_file,
		corine=shared_input_path / "geodata/onshore/corine.tif",
	output:
        # FIXME the file indreg is only necessary when we run on grid cell
        # currently when we run with region, we create an empty file
        # the cleaner solution might be to outsource the creation of that file
        # to an extra rule and have an input function conditional on the
        # wildcard spatial for that rule
		cf_exclusion_solar=temp(modelpath / "cf_exclusion_solar.tif"),
		cf_exclusion_windon=temp(modelpath / "cf_exclusion_windon.tif"),
		cf_exclusion_windoff=temp(modelpath / "cf_exclusion_windoff.tif"),
		indreg=temp(modelpath / "indices_region.csv"),
		grid_areassolar = modelpath / "grid_areas_solar.csv",
		grid_areaswindonshore=modelpath / "grid_areas_wind_onshore.csv",
		grid_areaswindoffshore=modelpath / "grid_areas_wind_offshore.csv",
	resources:
		mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
	threads: workflow.cores * 0.75
	params:
		sharedinputpath=shared_input_path,
		aggregated_regions=aggregated_regions,
		cutoffs=cutoff_values,
	notebook:
		"notebooks/highRES-build_vre_land_avail.ipynb"
		
rule build_vre_cf_inputs:
	input:
		cf_file=shared_input_path / "intermediate_data/europe_cf_{year}.nc",
		grid_areassolar = rules.build_vre_land_avail.output.grid_areassolar,
		grid_areaswindonshore = rules.build_vre_land_avail.output.grid_areaswindonshore,
		grid_areaswindoffshore = rules.build_vre_land_avail.output.grid_areaswindoffshore,
	output:
		capfacfile=temp(modelpath / "capacity-factors_solar_{year}.csv"),
		areassolar=temp(modelpath / "areas_solar.csv"),
		areaswindonshore=temp(modelpath / "areas_wind_onshore.csv"),
		areaswindoffshore=temp(modelpath / "areas_wind_offshore.csv"),
	params:
		date_range=date_range,
	notebook:
		"notebooks/highRES-build_vre_cf_inputs.ipynb"



rule build_hydro_capfac:
	input:
		eiahydrogen="resources/EIA_hydro_generation_1995_2000_2014.csv",
		hydroinstalledcap="resources/hydro_installed_cap.tsv",
		euroshape=shared_input_path
		/ "geodata/onshore/shapes/NUTS_RG_60M_2021_4326.geojson",
		weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
	output:
		hydrororcapfac=temp(modelpath / "capacity-factors_hydro_{year}.csv"),
		hydroresinfl=temp(modelpath / "inflow_hydro-res_{year}.csv"),
		areashydro=temp(modelpath / "hydro_ror_area.csv"),
	conda:
		"envs/build_hydro_capfac.yml"
	params:
		sharedinputpath=shared_input_path,
		aggregated_regions=aggregated_regions,
		date_range=date_range,
	notebook:
		"notebooks/highRES_build_hydro.py.ipynb"


def regionsonlyforgrid(wildcards):
    returndict = {}
    if wildcards.spatial == "grid":
        returndict["indreg"] = rules.build_vre_land_avail.output.indreg
    return returndict


rule build_regions_file:
    input:
        "workflow/scripts/build_regions.py",
        unpack(regionsonlyforgrid),
        zonescsv="resources/zones.csv",
        #rules.build_weather.output.indreg,
    output:
        regionsdd=modelpath / "_regions.dd",
    script:
        "scripts/build_regions.py"


# def build_vre_areas_file_func(wildcards):
#    returndict = {}
#    if wildcards.neigh != "Extreme":
#        returndict['areaswindon'] = rules.build_weather.output.areaswindonshore
#    return returndict


rule build_vre_areas_file:
    input:
        #        unpack(build_vre_areas_file_func),
        areashydro=rules.build_hydro_capfac.output.areashydro,
        areassolar=rules.build_vre_cf_inputs.output.areassolar,
        areaswindon=rules.build_vre_cf_inputs.output.areaswindonshore,
        areaswindoff=rules.build_vre_cf_inputs.output.areaswindoffshore,
        vreareaheader="resources/vre_areas_header.txt",
        genericfooter="resources/generic_footer.txt",
    output:
        modelpath / "vre_areas_{year}_.dd",
        unsorted=temp(modelpath / "vre_areas_unsorted.txt"),
        areassorted=temp(modelpath / "vre_areas_sorted.txt"),
    script:
        "scripts/build_vre_areas_file.py"


# shell:
#     # TODO platform independence
#     (
#         "cat {input[areashydro]} {input[areassolar]} {input[areaswindon]} "
#         "{input[areaswindoff]} | sort -g | cat {input[vreareaheader]} - "
#         "{input[genericfooter]} > {output[0]}"
#     )
# """ run:
#     import shutil
#     with open('unsorted.txt','wb') as wfd:
#         for f in [input[areashydro],input[areassolar],input[areaswindon],input[areaswindoff]]:
#             with open(f,'rb') as fd:
#                 shutil.copyfileobj(fd, wfd) """


rule build_hydrores_inflow:
    input:
        rules.build_hydro_capfac.output.hydroresinfl,
    output:
        inflowgdx=modelpathyearonly / "hydro_res_inflow_{year}.gdx",
    shell:
        gamspath + (
            "csv2gdx {input} output={output} ID=hydro_inflow "
            "Index='(1,2,3)' Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_hydrores_inflow:
    input:
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        compressdone=touch(modelpathyearonly / "hydrores_inflow_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


rule link_hydrores_inflow:
    input:
        rules.compress_hydrores_inflow.output.compressdone,
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",
    run:
        # copy instead of link due to missing privileges on windows
        import shutil

        shutil.copy2(input[1], output[0])


rule build_vre_file:
    input:
        cfwsng=rules.build_vre_cf_inputs.output.capfacfile,
        cfhn3=rules.build_hydro_capfac.output.hydrororcapfac,
    output:
        vrefile=temp(modelpath / "vre_{year}_.csv"),
    run:
        import shutil

        with open(output.vrefile, "wb") as wfd:
            for f in [
                input["cfwsng"],
                input["cfhn3"],
            ]:
                with open(f, "rb") as fd:
                    shutil.copyfileobj(fd, wfd)
                    # shell:
                    #     # TODO platform independence
                    #     (
                    #         "cat {input[cfwsng]} {input[cfhn3]} | sed 's/bottom//'"
                    #         " > {output[vrefile]}"
                    #     )



rule build_vre_parquet:
    input:
        rules.build_vre_file.output.vrefile,
    output:
        modelpath / "vre_{year}_.parquet",
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_parquet.py"


rule build_vre_csv:
    input:
        modelpath / "vre_{year}_.parquet",
    output:
        csvgdx=temp(modelpath / "vre_{year}_tmp.csv"),
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_csv.py"


rule build_vre_gdx:
    input:
        rules.build_vre_csv.output.csvgdx,
    output:
        bigvregdx=temp(modelpath / "vre_{year}_.gdx"),
    shell:
        # TODO platform independence
        gamspath + (
            "csv2gdx {input} output={output} ID=vre_gen Index='(1,2,3)'"
            " Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_vre_gdx:
    input:
        rules.build_vre_gdx.output.bigvregdx,
    output:
        touch(modelpath / "vre_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


""" def inputfilelist(wildcards):
    returndict = {}
    returndict['regionsfile'] = modelpath / "_regions.dd"
    returndict['zonesfile'] = modelpath / "zones.dd"
    returndict['temporalfile'] = modelpath / "{year}_temporal.dd"
    returndict['vreareafile'] = modelpath / "vre_areas_{year}_.dd"
    returndict['demandfile'] = modelpath / "BASE_demand_{year}.dd"
    returndict['capfacfilecompressed'] = modelpath / "vre_gdx.compressed"
    returndict['capfacfile'] = modelpath / "vre_{year}_.gdx"
    returndict['hydroresinflowfile'] = modelpath / "hydro_res_inflow_{year}.gdx"
    returndict['co2budgetfile'] = modelpath / "BASE_co2_budget.dd"
    returndict['genparamsfile'] = modelpath / "BASE_gen.dd"
    returndict['storeparamsfile'] = modelpath / "BASE_store.dd"
    returndict['transparamsfile'] = modelpath / "trans.dd"
    return returndict """


rule build_inputs:
    input:
        # unpack(inputfilelist)
        modelpath / "_regions.dd",
        modelpath / "zones.dd",
        modelpath / "{year}_temporal.dd",
        modelpath / "vre_areas_{year}_.dd",
        modelpath / "BASE_demand_{year}.dd",
        modelpath / "vre_gdx.compressed",
        modelpath / "vre_{year}_.gdx",
        modelpath / "hydro_res_inflow_{year}.gdx",
        modelpath / "BASE_co2_budget.dd",
        modelpath / "BASE_gen.dd",
        modelpath / "BASE_store.dd",
        modelpath / "trans.dd",
    output:
        touch(modelpath / "inputs.finished"),


rule run_model:
    input:
        modelpath / "highres.gms",
        modelpath / "cplex.opt",
        shared_code_path + "highres_data_input.gms",
        shared_code_path + "highres_hydro.gms",
        shared_code_path + "highres_results.gms",
        shared_code_path + "highres_storage_setup.gms",
        shared_code_path + "highres_storage_uc_setup.gms",
        shared_code_path + "highres_uc_setup.gms",
        modelpath / "inputs.finished",
        modelpath / "vre_{year}_.gdx",
    params:
        gamspath=gamspath,
        modelpath=str(modelpath),
    # retries: 3
    log:
        str(modelpath) + "/highres.lst",
        str(modelpath) + "/highres.log",
    output:
        modelresults=protected(modelpath / "results.gdx"),
        modelresultsdd=protected(modelpath / "results.db"),
    script:
        # TODO platform independence
        "scripts/run_gams.sh"


rule convert_results:
    input:
        rules.run_model.output.modelresults,
    output:
        resultsdb=ensure(protected(modelpath / "results.db"), non_empty=True),
    shell:
        # TODO platform independence
        gamspath + "gdx2sqlite -i {input} -o {output} -fast"


rule convert_results_db_parquet:
    input:
        rules.run_model.output.modelresultsdd,
    output:
        compressdone=touch(modelpath / "results.db.compressed"),
    resources:
        mem_mb=50000,
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/convert_results_sqlite_parquet.py"
