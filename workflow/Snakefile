
configfile: "config/config_ci.yaml"


import itertools

import pandas as pd
import pathlib

date_range = config["parameters"]["date_range"]

user_mode = config["parameters"]["mode"][0]

inputyears = config["parameters"]["years"]

# TODO: make grid_regions work
grid_regions = []
aggregated_regions = config["parameters"]["aggregated_regions"]

# TODO: worry about zones vs regions later this only works for all aggregated
pd.DataFrame(aggregated_regions).rename(columns={0: "zone"}).to_csv(
    "resources/zones.csv", index=None
)
# TODO: maybe at some stage remove the need for this redundant file
pd.DataFrame(aggregated_regions).assign(
    Country="",
    ISO3="",
    ETM=1,
    ETM_reg="",
    EU_simpl="",
    gb_ext="",
    UTC="",
    JRC_EUTIMES="",
).rename(columns={0: "ISO2"}).loc[
    :,
    [
        "Country",
        "ISO3",
        "ISO2",
        "ETM",
        "ETM_reg",
        "EU_simpl",
        "gb_ext",
        "UTC",
        "JRC_EUTIMES",
    ],
].to_csv(
    "resources/europe_countries.csv", index=None
)

spatials = config["parameters"]["spatials"]

cutoff_values = {
    "solar": config["parameters"]["cutoff_solar"],
    "onwind": config["parameters"]["cutoff_onwind"],
    "offwind": config["parameters"]["cutoff_offwind"],
}

corine_codes = {
    "solar": config["parameters"]["solar_clc"],
    "onwind_no_buffer": config["parameters"]["onwind_clc_no_buffer"],
    "onwind_buffer": config["parameters"]["onwind_clc_buffer"],
}

scenarios = pd.DataFrame(inputyears, columns=["years"])

concatlist = []
for spatial in spatials:
    concatlist.append(scenarios.assign(spatials=spatial))
scenarios = pd.concat(concatlist).reset_index(drop=True)

years = scenarios.years.to_list()
spatials = scenarios.spatials.to_list()

bias_correction = config["parameters"]["bias_correction"]
elevation_excl = config["parameters"]["elevation_excl"]

# Get current working directory

cwd = pathlib.Path().resolve()

# Absolute path to your GAMS installation
gamspath = config["paths"]["gams"]
gamspath = str(gamspath)

# Absolute path to the input data that is the same across all versions
shared_input_path = cwd / pathlib.Path(config["paths"]["shared_input"])

# Relative path the model code that is the same across all versions
shared_code_path = config["paths"]["abs_shared_code"]

abs_gams_code_path= cwd / pathlib.Path(config["paths"]["abs_shared_code"])

# Relative path to the geodata files that differentiate the scenarios
scenario_exclusions_path = "resources/scenario_exclusions/"

# Relative path to the results
# TODO: Vetle: --directory, flag for Ã¥ sette results directory?
# resultspath = "results/"
resultspath = cwd / pathlib.Path(config["paths"]["results"])
pathlib.Path(resultspath).mkdir(parents=True, exist_ok=True)

# Write scenarios to file, so results analysis script can check if they are all
# available.
scenarios.to_csv(resultspath / "scenarios.csv", sep="\t")

# Relative path to the built model and built inputs
yearonlypart = "models/{year}/"
scenariopart = "{spatial}/"
modelpathyearonly = resultspath / yearonlypart
modelpath = modelpathyearonly / scenariopart
allyearpath = resultspath / "models/allyears/"
logpath = yearonlypart + scenariopart


localrules:
    all,
    build_cplex_opt,
    build_technoeconomic_inputs,
    rename_demand_file,
    build_zones_file,
    build_vre_areas_file,
    build_vre_file,
    build_vre_gdx,
    build_hydro_capfac,
    build_hydrores_inflow,
    link_hydrores_inflow,
    convert_results,
    build_vre_parquet,
    build_vre_csv,
    compress_vre_gdx,
    compress_hydrores_inflow,
    convert_results_db_parquet,
    run_gams,


rule all:
    input:
        expand(modelpath / config["target"], zip, year=years, spatial=spatials),


rule build_cplex_opt:
    output:
        modelpath / "cplex.opt",
    run:
        cplex_options = config["cplex_options"]
        
        # Open the file in write mode
        with open(output[0], "w") as f:
            for option, value in cplex_options.items():
                f.write(f"{option} {value}\n")



rule build_zones_file:
    input:
        "resources/zones.csv",
        "workflow/scripts/build_zones.py",
    output:
        modelpath / "zones.dd",
    script:
        "scripts/build_zones.py"


rule build_technoeconomic_inputs:
    input:
        "resources/zones.csv",
        "resources/highres_scenarios.xls",
        "resources/highres_technoeconomic_database.ods",
        europecountriescsvlocation="resources/europe_countries.csv",
        europedemandcsvlocation="resources/europe_demand.csv",
        data2dd="workflow/scripts/data2dd_funcs.py",
    conda:
        "envs/build_technoeconomic.yml"
    output:
        modelpath / "{year}_temporal.dd",
        modelpath / "BASE_gen.dd",
        modelpath / "BASE_store.dd",
        modelpath / "trans.dd",
        demandfile=(
            modelpath / "BASE_norescale_demand_{year}.dd"
            if user_mode == "developer"
            else temp(modelpath / "BASE_norescale_demand_{year}.dd")
        ),
    params:
        aggregated_regions=aggregated_regions,
        date_range=date_range,
        user_mode=user_mode,
    script:
        "scripts/gb_ext_data2dd.py"


rule rename_demand_file:
    input:
        rules.build_technoeconomic_inputs.output.demandfile,
    output:
        modelpath / "BASE_demand_{year}.dd",
    run:
        import shutil

        shutil.copy2(input[0], output[0])


rule build_shapes:
    input:
        euroshape=shared_input_path
        / "geodata/onshore/shapes/NUTS_RG_01M_2021_4326.geojson",
        eurooffshoreshape=shared_input_path
        / ("geodata/offshore/BOTTOM_MOUNTED_EUROPE_NUTS0" "_NORWAY_NUTS3.geojson"),
    output:
        onshoreshape="intermediate_data/{spatial}/shapes/europe_onshore.geojson",
        offshoreshape="intermediate_data/{spatial}/shapes/europe_offshore.geojson",
    params:
        aggregated_regions=aggregated_regions,
    notebook:
        "notebooks/highRES-build_shapes.ipynb"


rule build_vre_cf_grid:
    input:
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
        euroshape=rules.build_shapes.output.onshoreshape,
        eurooffshoreshape=rules.build_shapes.output.offshoreshape,
        biaswinddata=(
            shared_input_path / "weatherdata/ratio_gwa2_era5.nc"
                if bias_correction
                else []
        )
    output:
        cf_file="intermediate_data/{spatial}/weather/europe_cf_{year}.nc",
    resources:
        mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
    threads: workflow.cores * 0.75
    params:
        sharedinputpath=shared_input_path,
        aggregated_regions=aggregated_regions,
        bias_correction=bias_correction,
    notebook:
        "notebooks/highRES-build_vre_cf_grid.ipynb"


rule build_vre_land_avail:
    input:
        WDPA1a=shared_input_path / "geodata/onshore/WDPA_Ia_100.tiff",
        WDPA1b=shared_input_path / "geodata/onshore/WDPA_Ib_100.tiff",
        WDPA2=shared_input_path / "geodata/onshore/WDPA_II_100.tiff",
        WDPA3=shared_input_path / "geodata/onshore/WDPA_III_100.tiff",
        WDPA4=shared_input_path / "geodata/onshore/WDPA_IV_100.tiff",
        elevation=shared_input_path / "geodata/onshore/2000m.shp.zip",
        slope=shared_input_path / "geodata/onshore/euro_slope_40degs.tif",
        euroshape=rules.build_shapes.output.onshoreshape,
        eurooffshoreshape=rules.build_shapes.output.offshoreshape,
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
        cfdata=rules.build_vre_cf_grid.output.cf_file,
        corine=shared_input_path / "geodata/onshore/corine.tif",
    output:
        # FIXME the file indreg is only necessary when we run on grid cell
        # currently when we run with region, we create an empty file
        # the cleaner solution might be to outsource the creation of that file
        # to an extra rule and have an input function conditional on the
        # wildcard spatial for that rule
        cf_exclusion_solar=(
            modelpath / "cf_exclusion_solar.tif"
            if user_mode == "developer"
            else temp(modelpath / "cf_exclusion_solar.tif")
        ),
        cf_exclusion_windon=(
            modelpath / "cf_exclusion_windon.tif"
            if user_mode == "developer"
            else temp(modelpath / "cf_exclusion_windon.tif")
        ),
        cf_exclusion_windoff=(
            modelpath / "cf_exclusion_windoff.tif"
            if user_mode == "developer"
            else temp(modelpath / "cf_exclusion_windoff.tif")
        ),
        indreg=(
            modelpath / "indices_region.csv"
            if user_mode == "developer"
            else temp(modelpath / "indices_region.csv")
        ),
        grid_areas=modelpath / "grid_areas.csv",
    resources:
        mem_mb=50000,  # TODO: Find out how much this rule needs,
        # should be constant (regional/grid)
    threads: workflow.cores * 0.75
    params:
        sharedinputpath=shared_input_path,
        aggregated_regions=aggregated_regions,
        cutoffs=cutoff_values,
        corine_codes=corine_codes,
        elevation_excl=elevation_excl,
    notebook:
        "notebooks/highRES-build_vre_land_avail.ipynb"


rule build_vre_cf_inputs:
    input:
        cf_file=rules.build_vre_cf_grid.output.cf_file,
        grid_areas=rules.build_vre_land_avail.output.grid_areas,
    output:
        capfacfile=(
            modelpath / "capacity-factors_solar_{year}.csv"
            if user_mode == "developer"
            else temp(modelpath / "capacity-factors_solar_{year}.csv")
        ),
        areassolar=(
            modelpath / "areas_solar.csv"
            if user_mode == "developer"
            else temp(modelpath / "areas_solar.csv")
        ),
        areaswindonshore=(
            modelpath / "areas_wind_onshore.csv"
            if user_mode == "developer"
            else temp(modelpath / "areas_wind_onshore.csv")
        ),
        areaswindoffshore=(
            modelpath / "areas_wind_offshore.csv"
            if user_mode == "developer"
            else temp(modelpath / "areas_wind_offshore.csv")
        ),
    params:
        date_range=date_range,
    notebook:
        "notebooks/highRES-build_vre_cf_inputs.ipynb"


rule build_hydro_capfac:
    input:
        eiahydrogen="resources/EIA_hydro_generation_1995_2000_2014.csv",
        jrchydro="resources/jrc-hydro-power-plant-database.csv",
        euroshape=rules.build_shapes.output.onshoreshape,
        weatherdata=shared_input_path / "weatherdata/europe_{year}.nc",
    output:
        hydrororcapfac=(
            modelpath / "capacity-factors_hydro_{year}.csv"
            if user_mode == "developer"
            else temp(modelpath / "capacity-factors_hydro_{year}.csv")
        ),
        hydroresinfl=(
            modelpath / "inflow_hydro-res_{year}.csv"
            if user_mode == "developer"
            else temp(modelpath / "inflow_hydro-res_{year}.csv")
        ),
        areashydro=(
            modelpath / "hydro_ror_area.csv"
            if user_mode == "developer"
            else temp(modelpath / "hydro_ror_area.csv")
        ),
    conda:
        "envs/build_hydro_capfac.yml"
    params:
        sharedinputpath=shared_input_path,
        aggregated_regions=aggregated_regions,
        date_range=date_range,
    notebook:
        "notebooks/highRES_build_hydro.py.ipynb"


def regionsonlyforgrid(wildcards):
    returndict = {}
    if wildcards.spatial == "grid":
        returndict["indreg"] = rules.build_vre_land_avail.output.indreg
    return returndict


rule build_vre_areas_file:
    input:
        areashydro=rules.build_hydro_capfac.output.areashydro,
        areassolar=rules.build_vre_cf_inputs.output.areassolar,
        areaswindon=rules.build_vre_cf_inputs.output.areaswindonshore,
        areaswindoff=rules.build_vre_cf_inputs.output.areaswindoffshore,
        data2dd=pathlib.Path(workflow.basedir) / "scripts",
    output:
        modelpath / "vre_areas_{year}_.dd",
        regionsdd=modelpath / "_regions.dd",
    notebook:
        "notebooks/highRES_build_vre_areas_file.ipynb"



rule build_hydrores_inflow:
    input:
        rules.build_hydro_capfac.output.hydroresinfl,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",
    shell:
        gamspath + (
            "csv2gdx {input} output={output} ID=hydro_inflow "
            "Index='(1,2,3)' Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_hydrores_inflow:
    input:
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        compressdone=touch(modelpathyearonly / "hydrores_inflow_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


rule link_hydrores_inflow:
    input:
        rules.compress_hydrores_inflow.output.compressdone,
        rules.build_hydrores_inflow.output.inflowgdx,
    output:
        inflowgdx=modelpath / "hydro_res_inflow_{year}.gdx",
    run:
        # copy instead of link due to missing privileges on windows
        import shutil

        shutil.copy2(input[1], output[0])


rule build_vre_file:
    input:
        cfwsng=rules.build_vre_cf_inputs.output.capfacfile,
        cfhn3=rules.build_hydro_capfac.output.hydrororcapfac,
    output:
        vrefile=(
            modelpath / "vre_{year}_.csv"
            if user_mode == "developer"
            else temp(modelpath / "vre_{year}_.csv")
        ),
    run:
        import shutil

        with open(output.vrefile, "wb") as wfd:
            for f in [
                input["cfwsng"],
                input["cfhn3"],
            ]:
                with open(f, "rb") as fd:
                    shutil.copyfileobj(fd, wfd)
                    # shell:
                    #     # TODO platform independence
                    #     (
                    #         "cat {input[cfwsng]} {input[cfhn3]} | sed 's/bottom//'"
                    #         " > {output[vrefile]}"
                    #     )



rule build_vre_parquet:
    input:
        rules.build_vre_file.output.vrefile,
    output:
        modelpath / "vre_{year}_.parquet",
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_parquet.py"


rule build_vre_csv:
    input:
        modelpath / "vre_{year}_.parquet",
    output:
        csvgdx=(
            modelpath / "vre_{year}_tmp.csv"
            if user_mode == "developer"
            else temp(modelpath / "vre_{year}_tmp.csv")
        ),
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/build_vre_csv.py"


rule build_vre_gdx:
    input:
        rules.build_vre_csv.output.csvgdx,
    output:
        bigvregdx=(
            modelpath / "vre_{year}_.gdx"
            if user_mode == "developer"
            else temp(modelpath / "vre_{year}_.gdx")
        ),
    shell:
        # TODO platform independence
        gamspath + (
            "csv2gdx {input} output={output} ID=vre_gen Index='(1,2,3)'"
            " Value='(4)' UseHeader=True StoreZero=True"
        )


rule compress_vre_gdx:
    input:
        rules.build_vre_gdx.output.bigvregdx,
    output:
        touch(modelpath / "vre_gdx.compressed"),
    shell:
        gamspath + "gdxcopy -V7C -Replace {input}"


""" def inputfilelist(wildcards):
    returndict = {}
    returndict['regionsfile'] = modelpath / "_regions.dd"
    returndict['zonesfile'] = modelpath / "zones.dd"
    returndict['temporalfile'] = modelpath / "{year}_temporal.dd"
    returndict['vreareafile'] = modelpath / "vre_areas_{year}_.dd"
    returndict['demandfile'] = modelpath / "BASE_demand_{year}.dd"
    returndict['capfacfilecompressed'] = modelpath / "vre_gdx.compressed"
    returndict['capfacfile'] = modelpath / "vre_{year}_.gdx"
    returndict['hydroresinflowfile'] = modelpath / "hydro_res_inflow_{year}.gdx"
    returndict['co2budgetfile'] = modelpath / "BASE_co2_budget.dd"
    returndict['genparamsfile'] = modelpath / "BASE_gen.dd"
    returndict['storeparamsfile'] = modelpath / "BASE_store.dd"
    returndict['transparamsfile'] = modelpath / "trans.dd"
    return returndict """


rule build_inputs:
    input:
        # unpack(inputfilelist)
        modelpath / "_regions.dd",
        modelpath / "zones.dd",
        modelpath / "{year}_temporal.dd",
        modelpath / "vre_areas_{year}_.dd",
        modelpath / "BASE_demand_{year}.dd",
        modelpath / "vre_gdx.compressed",
        modelpath / "vre_{year}_.gdx",
        modelpath / "hydro_res_inflow_{year}.gdx",
        modelpath / "BASE_gen.dd",
        modelpath / "BASE_store.dd",
        modelpath / "trans.dd",
    output:
        touch(modelpath / "inputs.finished"),



rule run_gams:
    input:
        modelpath / "cplex.opt",
        shared_code_path + "highres.gms",
        shared_code_path + "highres_data_input.gms",
        shared_code_path + "highres_hydro.gms",
        shared_code_path + "highres_results.gms",
        shared_code_path + "highres_storage_setup.gms",
        shared_code_path + "highres_storage_uc_setup.gms",
        shared_code_path + "highres_uc_setup.gms",
        modelpath / "inputs.finished",
        modelpath / "vre_{year}_.gdx",
    params:
        gamspath=gamspath,
        modelpath=str(modelpath),
        co2intensity=config["parameters"]["co2target"][0],
        sharedcodepath=abs_gams_code_path,
        outname="results",
    resources:
        mem_mb=50000,
    log:
        str(modelpath) + "/highres.lst",
        str(modelpath) + "/highres.log",
    output:
        modelresults=protected(modelpath / "results.gdx"),
        # modelresultsdd=protected(modelpath / "results.db"),
    script:
        "scripts/run_gams.py"


rule convert_results:
    input:
        rules.run_gams.output.modelresults,
    output:
        resultsdb=ensure(protected(modelpath / "results.db"), non_empty=True),
    shell:
        # TODO platform independence
        gamspath + "gdx2sqlite -i {input} -o {output} -fast"


rule convert_results_db_parquet:
    input:
        rules.convert_results.output.resultsdb,
    output:
        compressdone=touch(modelpath / "results.db.compressed"),
    resources:
        mem_mb=50000,
    conda:
        "envs/build_vre_parquet.yml"
    script:
        "scripts/convert_results_sqlite_parquet.py"
