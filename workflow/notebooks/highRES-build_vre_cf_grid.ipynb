{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d46045-f1d3-4ef4-98c9-05eca2927da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharedinputpath = snakemake.params[\"sharedinputpath\"]\n",
    "desired_regions = snakemake.params.aggregated_regions\n",
    "weatherdata = snakemake.input.weatherdata\n",
    "\n",
    "cf_file = snakemake.output.cf_file\n",
    "\n",
    "\n",
    "cmip_windspeed = snakemake.params.windspeed\n",
    "cmip_solar = snakemake.params.solar\n",
    "cmip_year = snakemake.wildcards.cmip_year\n",
    "cmip_model = snakemake.wildcards.cmip_model\n",
    "cmip_scenario = snakemake.wildcards.cmip_scenario\n",
    "era5_year = int(snakemake.wildcards.year)\n",
    "pv_type_pecd = snakemake.params.pv_type_pecd\n",
    "\n",
    "cmip_windspeed_path = snakemake.params.windspeed_path\n",
    "cmip_solar_cf_path = snakemake.params.solar_cf_path\n",
    "\"/fp/projects01/ec285/Europe_v1/data/PECD/v4.2/ERA5/100m_wind_speed/\"\n",
    "# path to cmip files based on cmip year and cmip model\n",
    "if cmip_model==\"ERA5\":\n",
    "    cmip_windspeed_file_path = f\"{cmip_windspeed_path}/ERA5/100m_wind_speed/H_ERA5_ECMW_T639_WS-_0100m_Pecd_025d_S{cmip_year}*\"\n",
    "    cmip_solar_cf_file_path = f\"{cmip_solar_cf_path}/ERA5/solar_photovoltaic_generation_capacity_factor/{pv_type_pecd}/H_ERA5_ECMW_T639_SPV_0000m_Pecd_025d_S{cmip_year}*\"\n",
    "else:\n",
    "    cmip_windspeed_file_path = f\"{cmip_windspeed_path}/{cmip_scenario}/100m_wind_speed/P_CMI6_{cmip_model}_WS-_0100m_Pecd_025d_S{cmip_year}*\"\n",
    "    cmip_solar_cf_file_path = f\"{cmip_solar_cf_path}/{cmip_scenario}/solar_photovoltaic_generation_capacity_factor/{pv_type_pecd}/P_CMI6_{cmip_model}_SPV_0000m_Pecd_025d_S{cmip_year}*\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcfa05-08af-45bf-b050-98527ffb6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "onshore_turbine = snakemake.params.windturbines.get('onshore')\n",
    "offshore_bottom_turbine = snakemake.params.windturbines.get('offshore_bottom')\n",
    "offshore_floating_turbine = snakemake.params.windturbines.get('offshore_float')\n",
    "\n",
    "panel = \"CSi\"\n",
    "orientation = \"latitude_optimal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae506d77-a5a8-4cbf-9414-3dc84e678ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata_files = {\n",
    "    \"onshore\": snakemake.input.euroshape,\n",
    "    \"offshore_bottom\": snakemake.input.eurooffshoreshape,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375303b-77e1-4ca8-8d3b-57f99d4493e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import atlite\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c751ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_swap_spatial_dims(ds, namex=\"x\", namey=\"y\"):\n",
    "    \"\"\"\n",
    "    Swap order of spatial dimensions according to atlite concention.\n",
    "    \"\"\"\n",
    "    swaps = {}\n",
    "    lx, rx = ds.indexes[namex][[0, -1]]\n",
    "    ly, uy = ds.indexes[namey][[0, -1]]\n",
    "\n",
    "    if lx > rx:\n",
    "        swaps[namex] = slice(None, None, -1)\n",
    "    if uy < ly:\n",
    "        swaps[namey] = slice(None, None, -1)\n",
    "\n",
    "    return ds.isel(**swaps) if swaps else ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2e16c-f864-46f7-97b0-a54b08a2b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%psource atlite.Cutout.convert_and_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6967d-f0c0-4005-8de8-21061b4b3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = []\n",
    "for geodata_file_name, geodata_file_path in geodata_files.items():\n",
    "    print(geodata_file_path)\n",
    "    boundaries.append(gpd.read_file(geodata_file_path))\n",
    "\n",
    "boundaries = pd.concat(boundaries).bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca156b3c-1cec-478d-b000-0fae57202a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = boundaries.groupby(lambda x: \"bountry\").agg(\n",
    "    {\"minx\": \"min\", \"miny\": \"min\", \"maxx\": \"max\", \"maxy\": \"max\"}\n",
    ")\n",
    "\n",
    "# boundaries[\"minx\"] = boundaries[\"minx\"] - 2\n",
    "# boundaries[\"miny\"] = boundaries[\"miny\"] - 2\n",
    "# boundaries[\"maxx\"] = boundaries[\"maxx\"] + 2\n",
    "# boundaries[\"maxy\"] = boundaries[\"maxy\"] + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29008a6-bd0d-45b2-b916-2def343bb417",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(weatherdata, chunks=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c100cf-894c-4488-929a-57885aa6b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout = atlite.Cutout(\n",
    "    path=\"../3_intermediate_data/intermediatecutout.nc\",\n",
    "    data=ds.sel(\n",
    "        x=slice(\n",
    "            boundaries.loc[\"bountry\", \"minx\"],\n",
    "            boundaries.loc[\"bountry\", \"maxx\"],\n",
    "        ),\n",
    "        y=slice(\n",
    "            boundaries.loc[\"bountry\", \"miny\"],\n",
    "            boundaries.loc[\"bountry\", \"maxy\"],\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "cutout.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b257c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 wind speed from PECD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969856c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if cmip_windspeed:\n",
    "    pan = xr.open_mfdataset(cmip_windspeed_file_path)\n",
    "    pan = maybe_swap_spatial_dims(pan, namex=\"longitude\", namey=\"latitude\")\n",
    "\n",
    "    # lat and lon for the new cutout (smaller than the original) - hardcoded\n",
    "    lat_min = 32.0\n",
    "    lat_max = 75.0\n",
    "    lon_min = -13.0\n",
    "    lon_max = 45.0\n",
    "\n",
    "    cutout = cutout.sel(y=slice(lat_min,lat_max), x=slice(lon_min,lon_max))\n",
    "\n",
    "    # create wind speed cutout from pan-european dataset\n",
    "    wnd100m = (\n",
    "        pan\n",
    "        .rename({\"longitude\":\"x\",\"latitude\":\"y\"})\n",
    "        # .drop_vars(\"height\") # v4.1\n",
    "        .sel(time=slice(cmip_year,cmip_year))\n",
    "        .rename({\"ws100\":\"wnd100m\"})\n",
    "        [\"wnd100m\"]\n",
    "    )\n",
    "    wnd100m = (\n",
    "        wnd100m\n",
    "        .assign_coords(lon=wnd100m.coords[\"x\"], lat=wnd100m.coords[\"y\"])\n",
    "    )\n",
    "    wnd100m = wnd100m.sel(time=~((wnd100m.time.dt.month == 2) & (wnd100m.time.dt.day == 29))) #removing 29-feb (leap-year)\n",
    "\n",
    "    # replace 2050 by 2010\n",
    "    new_time_index = pd.DatetimeIndex([pd.Timestamp(t).replace(year=era5_year) for t in wnd100m.time.values])\n",
    "    wnd100m[\"time\"] = new_time_index\n",
    "    # add attributes from era5 wnd100m\n",
    "    # wnd100m = wnd100m.assign_attrs(era5_old[\"wnd100m\"].attrs)\n",
    "    attrs = cutout.data[\"wnd100m\"].attrs\n",
    "    # select the specific cutout from the pan-european\n",
    "    wnd100m = wnd100m.sel(\n",
    "        time=cutout.data.time, \n",
    "        x=cutout.data.x, \n",
    "        y=cutout.data.y, \n",
    "        method=\"nearest\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create new data array\n",
    "    new_variable = xr.DataArray(\n",
    "        data=wnd100m.values,  \n",
    "        dims=wnd100m.dims,\n",
    "        coords=wnd100m.coords,\n",
    "        attrs=attrs  \n",
    "    )\n",
    "\n",
    "    # Add the variable to era5_pan\n",
    "    cutout.data['wnd100m'] = new_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind bias-correction - 100m wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if snakemake.params.bias_correction:\n",
    "    # Path to bias correction ratios\n",
    "    windbiaspath = snakemake.input.biaswinddata\n",
    "\n",
    "    # Get wind speed at 100m from cutout\n",
    "    wnd_100m = cutout.data[\"wnd100m\"]\n",
    "    attrs = wnd_100m.attrs\n",
    "\n",
    "    # Load dataset containing bias correction ratios\n",
    "    gwa2_ratio = xr.open_dataset(windbiaspath, chunks=\"auto\")\n",
    "\n",
    "    # Select bias correction ratio for wind speed at 100m and\n",
    "    # rename coordinates to the same names in cutout\n",
    "    gwa2_ratio_100m = (\n",
    "        gwa2_ratio\n",
    "        # .sel(height=100, drop=True)\n",
    "        # .drop_vars(\"spatial_ref\")\n",
    "        .rename({\"longitude\": \"x\", \"latitude\": \"y\"})\n",
    "    )\n",
    "\n",
    "    # Actual bias-correction ratios (from GWA2) has finner resolution\n",
    "    # than ERA5 data (bias-correction ratios: 0.025, and ERA5: 0.25)\n",
    "    # To match both data, we aggregate (average) ratios to ERA5 grid cells.\n",
    "    # Also, the coordinates in both datasets do not match. So, we\n",
    "    # interpolated according to ERA5 grid cells.\n",
    "    # TODO: In the future, we can downscale ERA5 wind speed data to GWA2\n",
    "    # resolution, exclude certain grid cells to make a more sophisticated\n",
    "    # filter, and then upscale to the original ERA5 resolution.\n",
    "\n",
    "    # For now, the code aggregates the GWA2 ratio to the ERA5 resolution\n",
    "    # (from 0.025 to 0.25), interpolates to ERA5 grid cells, and then\n",
    "    # corrects the wind speed.\n",
    "\n",
    "    # Calculate number of points to aggregate\n",
    "\n",
    "    # steps dx and dy (GWA2)\n",
    "    dx_gwa2 = 0.025\n",
    "    dy_gwa2 = 0.025\n",
    "    # ERA5\n",
    "    dx_era5 = 0.25\n",
    "    dy_era5 = 0.25\n",
    "\n",
    "    # Another way\n",
    "    # dx_gwa2 = np.round(gwa2_ratio_100m.coords['x'].diff('x').values[0],4)\n",
    "    # dy_gwa2 = np.round(gwa2_ratio_100m.coords['y'].diff('y').values[0],4)\n",
    "\n",
    "    # dx_era5 = np.round(wnd_100m.coords['x'].diff('x').values[0],4)\n",
    "    # dy_era5 = np.round(wnd_100m.coords['y'].diff('y').values[0],4)\n",
    "\n",
    "    # Calculate the number of points to aggregate when the data is coarsened\n",
    "    x_window = int(dx_era5 / dx_gwa2)\n",
    "    y_window = int(dy_era5 / dy_gwa2)\n",
    "\n",
    "    # Coarsen and interpolate gwa2 ratios resolution (0.025)\n",
    "    # to the ERA5 resolution (0.25) by averaging\n",
    "    gwa2_coarsened = (\n",
    "        gwa2_ratio_100m.coarsen(x=x_window, y=y_window, boundary=\"pad\")\n",
    "        .mean()\n",
    "        .interp_like(wnd_100m)\n",
    "    )\n",
    "\n",
    "    # Correct 100m wind speed\n",
    "    cutout.data[\"wnd100m\"] = (\n",
    "        wnd_100m * gwa2_coarsened[\"ratio_gwa2_era5_mean_WS\"]\n",
    "    ).assign_attrs(attrs)  # copy also the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7663b5-aee3-4c09-a065-188d1920da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd466b5-508f-4f0d-a2dd-e39201e0f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "%psource atlite.Cutout.convert_and_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74616fc-ab4b-4d93-a3aa-1f35b11f50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_solar = cutout.pv(\n",
    "    panel=panel,\n",
    "    orientation=orientation,\n",
    "    capacity_factor_timeseries=True,\n",
    ").astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b500b-3d4e-4423-ace9-9225d6230da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Onshore wind CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94b3ab-1ade-499c-a297-e6ea5a9ade73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_windon = cutout.wind(\n",
    "    turbine=onshore_turbine, \n",
    "    capacity_factor_timeseries=True,\n",
    "    smooth=snakemake.params.wind_smooth\n",
    ").astype(np.float32)*snakemake.params.windon_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc097f5d-659f-4726-a711-8bac119180d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offshore wind CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd819dc5-c468-4bb2-94e2-b74e9edb3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_windoff_bottom = cutout.wind(\n",
    "    turbine=offshore_bottom_turbine, \n",
    "    capacity_factor_timeseries=True,\n",
    "    smooth=snakemake.params.wind_smooth\n",
    ").astype(np.float32)*snakemake.params.windoff_bottom_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb08cf4-7890-4191-b6e1-e0546a591679",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = xr.concat(\n",
    "    [cf_solar, cf_windon, cf_windoff_bottom],\n",
    "    pd.Index([\"Solar\", \"Windonshore\", \"Windoffshore\"], name=\"Tech\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar capacity factors PECD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solar capacity factors from PECD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbf4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if cmip_solar:\n",
    "\n",
    "    # open dataset\n",
    "    pan = xr.open_mfdataset(cmip_solar_cf_file_path)\n",
    "    pan = maybe_swap_spatial_dims(pan, namex=\"longitude\", namey=\"latitude\")\n",
    "\n",
    "    # get solar capacity factors and put in correct format\n",
    "    solar = (\n",
    "        pan\n",
    "        .rename({\"longitude\":\"x\",\"latitude\":\"y\"})\n",
    "        .rename({\"spv_cf\":\"capacity factor\"})\n",
    "        .sel(time=slice(cmip_year,cmip_year))\n",
    "    )\n",
    "\n",
    "    solar = (\n",
    "        solar\n",
    "        .assign_coords(lon=solar.coords[\"x\"], lat=solar.coords[\"y\"])\n",
    "        [\"capacity factor\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    # lat and lon for the new cutout (smaller than the original) - hardcoded values\n",
    "    lat_min = 35.0\n",
    "    lat_max = 71.0\n",
    "    lon_min = -11.5\n",
    "    lon_max = 31.5\n",
    "\n",
    "    cf_pan = cf.where(\n",
    "        (cf.lat >= lat_min) & (cf.lat <= lat_max) &\n",
    "        (cf.lon >= lon_min) & (cf.lon <= lon_max),\n",
    "        drop=True\n",
    "    )\n",
    "    solar = solar.sel(time=~((solar.time.dt.month == 2) & (solar.time.dt.day == 29))) #removing 29-feb (leap-year)\n",
    "    # replace 2050 by 2010\n",
    "    new_time_index = pd.DatetimeIndex([pd.Timestamp(t).replace(year=era5_year) for t in solar.time.values])\n",
    "    solar[\"time\"] = new_time_index\n",
    "\n",
    "    # add attributes from era5 wnd100m\n",
    "    # wnd100m = wnd100m.assign_attrs(era5_old[\"wnd100m\"].attrs)\n",
    "    attrs = cf.sel(Tech = \"Solar\").attrs\n",
    "    # select the specific cutout from the pan-european\n",
    "    solar = solar.sel(\n",
    "        time=cf_pan.time, \n",
    "        x=cf_pan.x, \n",
    "        y=cf_pan.y, \n",
    "        method=\"nearest\"\n",
    "    )\n",
    "\n",
    "    # Create new data array\n",
    "    new_variable = xr.DataArray(\n",
    "        data=solar.values,  \n",
    "        dims=solar.dims,\n",
    "        coords=solar.coords,\n",
    "        attrs=attrs  \n",
    "    )\n",
    "\n",
    "    new_variable = new_variable.to_dataset(name=\"capacity factor\")\n",
    "\n",
    "    # Put dimensions in the same order as era5 capacity factors\n",
    "    new_variable_reordered = new_variable[\"capacity factor\"].transpose(\"y\", \"time\", \"x\")\n",
    "\n",
    "    # Replace new variable\n",
    "    cf_pan.loc[dict(Tech=\"Solar\")] = new_variable_reordered\n",
    "\n",
    "    cf = cf_pan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5eafb-ec34-4b75-a70f-a93232780570",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.to_netcdf(\n",
    "    cf_file,\n",
    "    encoding={\n",
    "        \"capacity factor\": {\n",
    "            \"dtype\": \"int16\",\n",
    "            \"scale_factor\": 0.001,\n",
    "            \"_FillValue\": -99,\n",
    "            \"zlib\": True,\n",
    "            \"complevel\": 1,\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
